# Построение бинарного классификатора токсичных комментариев с использованием логистической регрессии

## Описание проекта  
В этом проекте разработан бинарный классификатор для определения токсичности комментариев.  

**Цель:** обучить модель, способную предсказывать, является ли комментарий токсичным (`1`) или нет (`0`).  

## Данные  
Датасет взят с **Kaggle (Jigsaw Toxic Comment Classification Challenge)** и включает:  

- **train.csv** – обучающие данные с текстами и метками (токсичность по категориям).  
- **test.csv** – тестовые данные без меток.  
- **test_labels.csv** – реальные метки для тестовых данных (но часть из них `-1` и не используется).  

## Шаги  
### Предобработка данных  
- Объединение категориальных меток в один бинарный признак (`label`).  
- Очистка текста: токенизация, лемматизация, удаление стоп-слов.  
- Балансировка данных с **undersampling**, чтобы уменьшить дисбаланс классов.  

### Обучение модели  
- Векторизация текста с помощью **TF-IDF**.  
- Обучение **логистической регрессии**.  
- Разделение данных на обучающую и тестовую выборки.  

### Оценка модели  
- Accuracy, Precision, Recall, F1-score.  
- Проверка на тестовом наборе (`test.csv`).  

## Результаты  
- **Accuracy** на тестовых данных: **85.6%**.  
- **Высокий recall (92%)** для токсичных комментариев (модель хорошо их находит).  
- **Precision (40%) для токсичных комментариев** – модель иногда ошибочно помечает нетоксичные комментарии как токсичные.  

## Вывод  
Модель хорошо классифицирует комментарии, но требует улучшения **точности** предсказания токсичности.  
Для дальнейшего развития можно попробовать **нейросети (BERT, LSTM)** или **XGBoost**.  
